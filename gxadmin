#!/bin/bash
usage(){
	if (( $# > 0 )) && [[ $1 != "safe" ]]; then
		error $@
	fi
	echo "gxadmin usage:"
	cmds="$(grep -o ' ## .*' $0 | grep -v grep | sort | sed 's/^ ## //g')"
	echo
	echo "Zergling Commands:"
	echo
	echo "$cmds" | grep '^zerg' | column -s: -t | sed 's/^/    /'
	echo
	echo "Handler Commands:"
	echo
	echo "$cmds" | grep '^handler' | column -s: -t | sed 's/^/    /'
	echo
	echo "DB Queries:"
	echo "  'query' can be exchanged with 'tsvquery' or 'csvquery' for tab- and comma-separated variants"
	echo
	echo "$cmds" | grep 'query ' | sort -k2 | column -s: -t | sed 's/^/    /'
	echo
	echo "Other:"
	echo
	echo "$cmds" | grep -v 'query ' | grep -v '^zerg' | grep -v handler | column -s: -t | sed 's/^/    /'
	echo
	echo "help / -h / --help : this message. Invoke '--help' on any subcommand for help specific to that subcommand"
	if [[ $1 == "safe" ]]; then
		exit 0;
	fi
	exit 1
}

handle_help() {
	for i in "$@"; do
		if [[ $i = --help || $i = -h ]]; then

			key="${mode}"
			if [[ ! -z "${subfunc}" ]]; then
				key="${key} ${subfunc}"
			fi

			echo "**NAME**"
			echo
			invoke_desc=$(grep "## ${key}[ :]" $0 | sed "s/.*## /gxadmin /g")
			short_desc=$(echo $invoke_desc | sed 's/.*://g')
			short_parm=$(echo $invoke_desc | sed 's/:.*//g')
			echo "${key} - ${short_desc}"
			echo
			echo "**SYNOPSIS**"
			echo
			echo $short_parm
			echo
			manual="$(cat -)"
			manual_wc="$(echo $manual | wc -c)"
			if (( manual_wc > 3 )); then
				echo "**NOTES**"
				echo
				echo "$manual"
				echo
			fi
			# exit after printing the documentation!
			exit 42
		fi
	done
}

assert_restart_lock(){
	if [ -f /usr/local/galaxy/galaxy-dist/.restart-lock ]; then
		echo "A restart lock exists. This means someone is probably already restarting galaxy."
		exit 3
	fi
}

error() {
	echo -e "\e[48;5;09m$@\e[m"
}

success() {
	echo -e "\e[38;5;40m$@\e[m"
}

validate() {
	if [[ -z "$GALAXY_CONFIG_DIR" ]]; then
		error Please set \$GALAXY_CONFIG_DIR
		exit 1
	fi
	if [[ -z "$GALAXY_MUTABLE_CONFIG_DIR" ]]; then
		error Please set \$GALAXY_MUTABLE_CONFIG_DIR
		exit 1
	fi

	fail_count=0
	for file in ${GALAXY_CONFIG_DIR}/*.xml; do
		xmllint $file > /dev/null 2>/dev/null;
		exit_code=$?
		if (( $exit_code > 0 )); then
			fail_count=$(echo "$fail_count + 1" | bc)
			error "  FAIL: $file ($exit_code)";
		else
			success "  OK: $file";
		fi
	done;

	for file in ${GALAXY_MUTABLE_CONFIG_DIR}/*.xml; do
		xmllint $file > /dev/null 2>/dev/null;
		exit_code=$?
		if (( $exit_code > 0 )); then
			fail_count=$(echo "$fail_count + 1" | bc)
			error "  FAIL: $file ($exit_code)";
		else
			success "  OK: $file";
		fi
	done;

	if (( fail_count == 0 )); then
		success "All XML files validated"
	else
		error "XML validation failed, cancelling any actions."
		exit 1
	fi
}

zerg_swap() {
	# Ensure that we lock out other users.
	assert_restart_lock

	zerg0running=0
	supervisorctl status gx:zergling0 | grep RUNNING
	zerg0=$?
	if (( $zerg0 == 0 )); then
		echo "zerg 0 is running"
		zerg0running=1
	fi

	zerg1running=0
	supervisorctl status gx:zergling1 | grep RUNNING
	zerg1=$?
	if [ "$zerg1" -eq "0" ]; then
		echo "zerg 1 is running"
		zerg1running=1
	fi

	if [[ "$zerg1running" -eq "1" && "$zerg0running" -eq "1" ]] ; then
		error "ERROR: BOTH ARE RUNNING"
		exit 2
	fi
	if [[ "$zerg1running" -eq "0" && "$zerg0running" -eq "0" ]] ; then
		error "ERROR: NEITHER ARE RUNNING"
		exit 3
	fi

	supervisorctlstatus=`supervisorctl status | grep zergling | sed 's/\s\+/ /g'`

	echo "Ok, everything looks good."
	# And again even if everything looks good, just in case.

	StartDate=`date "+%s"`
	touch  /usr/local/galaxy/galaxy-dist/.restart-lock

	# Ok, so now we are sure that just one is running
	if [ "$zerg1running" -eq "1" ] ; then
		echo "Starting zergling 0"
		supervisorctl start gx:zergling0
		echo "Zergling 0 should be running. Now wait patiently while it starts. This script will continue when it is ready."
		while [ $(curl --silent localhost:9190 | wc -c) -eq "0" ]; do
			sleep 5;
			echo -n '.'
		done
		echo
		supervisorctl stop gx:zergling1
	elif [ "$zerg0running" -eq "1" ] ; then
		echo "Starting zergling 1"
		supervisorctl start gx:zergling1
		echo "Zergling 1 should be running. Now wait patiently while it starts. This script will continue when it is ready."
		while [ $(curl --silent localhost:9191 | wc -c) -eq "0" ]; do
			sleep 5;
			echo -n '.'
		done
		echo
		supervisorctl stop gx:zergling0
	fi

	FinalDate=`date "+%s"`
	timing=`date -u -d "0 $FinalDate seconds - $StartDate seconds" +"%H:%M:%S"`
	success "Swap took $timing" "galaxy,zergling"
	rm /usr/local/galaxy/galaxy-dist/.restart-lock
}

supervisor_strace() {
	task=$1
	state=$(supervisorctl status | grep $task)
	echo "$state" | grep --quiet "RUNNING"
	if (( $? > 0 )); then
		error "$task is not running"
		exit 1
	fi

	pid=$(echo $state | egrep -o 'pid ([0-9]*)' | sed 's/pid //g' | tr '\n' ' ')
	pids=$(echo "$pid"  | sed 's/^\s*//g;s/\s*$//g;s/ / -p /g')
	strace -e open -p $pids
}

assert_count() {
	if (( $1 != $2 )); then
		error "$3"
		usage
		exit 1
	fi
}

assert_count_ge() {
	if (( $1 < $2 )); then
		error "$3"
		usage
		exit 1
	fi
}

query_tbl() {
	psql -c "$1"
}

query_tsv() {
	psql -c "COPY ($1) to STDOUT with CSV DELIMITER E'\t'"
}

query_csv() {
	psql -c "COPY ($1) to STDOUT with CSV DELIMITER ','"
}

query_influx() {
	arr2py=$(cat <<EOF
import sys
query_name = sys.argv[1]
fields = {x.split('=')[0]: int(x.split('=')[1]) for x in sys.argv[2].split(';')}
tags = []
if len(sys.argv) > 3:
	tags = {x.split('=')[0]: int(x.split('=')[1]) for x in sys.argv[3].split(';')}
for line in sys.stdin.read().split('\n'):
	if len(line) == 0:
		continue
	parsed = line.split(',')
	metric = query_name
	if len(tags):
		tag_data = ['%s=%s' % (k, parsed[v])  for (k, v) in tags.items()]
		metric += ',' + ','.join(tag_data)
	field_data = ['%s=%s' % (k, parsed[v])  for (k, v) in fields.items()]
	metric += ' ' + ','.join(field_data)
	print(metric)
EOF
)

	psql -c "COPY ($1) to STDOUT with CSV DELIMITER E','" | python -c "$arr2py" "$2" "$3" "$4"
}

if (( $# == 0 )); then
	usage safe
fi


##############################################################
# Functions
##############################################################

func_validate() { ## validate: validate config files
	handle_help "$@" <<-EOF
		Validate the configuration files
		**Warning**:
		- This requires you to have \`\$GALAXY_DIST\` set and to have config under \`\$GALAXY_DIST/config\`.
		- This only validates that it is well formed XML, and does **not** validate against any schemas.

		    $ gxadmin validate
		      OK: /usr/local/galaxy/galaxy-dist/data_manager_conf.xml
		      ...
		      OK: /usr/local/galaxy/galaxy-dist/config/tool_data_table_conf.xml
		      OK: /usr/local/galaxy/galaxy-dist/config/tool_sheds_conf.xml
		    All XML files validated
	EOF

	validate;
}

cleanup() { ## cleanup [days]: Cleanup histories/hdas/etc for past N days (default=30)
	handle_help "$@" <<-EOF
		Cleanup histories/hdas/etc for past N days using the python objects-based method
	EOF

	days=30
	if (( $# > 0 )); then
		days=$1
	fi

	# TODO(hxr): nicer syntax?
	if [[ -z "$GALAXY_ROOT" ]]; then
		error Please set \$GALAXY_ROOT
		exit 1
	fi
	if [[ -z "$GALAXY_CONFIG_FILE" ]]; then
		error Please set \$GALAXY_CONFIG_FILE
		exit 1
	fi
	if [[ -z "$GALAXY_LOG_DIR" ]]; then
		error Please set \$GALAXY_LOG_DIR
		exit 1
	fi

	for action in {delete_userless_histories,delete_exported_histories,purge_deleted_histories,purge_deleted_hdas,delete_datasets,purge_datasets}; do
		python $GALAXY_ROOT/scripts/cleanup_datasets/pgcleanup.py \
			-c $GALAXY_CONFIG_FILE \
			-o $days \
			-l $GALAXY_LOG_DIR \
			-s $action \
			-w 128MB 2>&1 >> $GALAXY_LOG_DIR/cleanup_datasets.log;

		# Something that telegraf can consume
		ec=$?
		if (( ec == 0 )); then
			echo "cleanup_datasets,group=$action success=1"
		else
			echo "cleanup_datasets,group=$action success=0"
		fi
	done
}

func_zerg_swap() { ## zerg swap <message>: swap zerglings
	handle_help "$@" <<-EOF
	EOF

	validate
	#assert_count $# 1 "Restart now requires a message"
	zerg_swap $1
}

func_zerg_tail() { ## zerg tail: tail zergling logs
	handle_help "$@" <<-EOF
	EOF

	tail -f $GALAXY_LOG_DIR/*zerg*.log
}

zerg_strace() { ## zerg strace [0|1|pool]: swap zerglings
	handle_help "$@" <<-EOF
	EOF

	param="$1";
	shift;
	if [[ $param == "pool" ]]; then
		supervisor_strace "gx:zergpool"
	elif [[ $param == "0" ]] || [[ $param == "1" ]]; then
		supervisor_strace "zergling$param"
	else
		supervisor_strace "gx:zerg"
	fi
}

zerg() {
	subfunc="$1"; shift
	case "$subfunc" in
		swap   ) func_zerg_swap  "$@";;
		tail   ) func_zerg_tail    "$@";;
		strace ) zerg_strace "$@";;
	esac
}

handler_strace() { ## handler strace <handler_id>: Run an strace on a specific handler (to watch it load files.)
	handle_help "$@" <<-EOF
	EOF

	supervisor_strace "hd:handler$1"
}

handler_tail() { ## handler tail: tail handler logs
	handle_help "$@" <<-EOF
	EOF

	tail -f $GALAXY_LOG_DIR/handler*.log
}

handler_restart() { ## handler restart <message>: restart handlers
	handle_help "$@" <<-EOF
	EOF

	validate
	supervisorctl restart hd:
}

handler() {
	subfunc="$1"; shift
	case "$subfunc" in
		strace  ) handler_strace  "$@";;
		tail    ) handler_tail    "$@";;
		restart ) handler_restart "$@";;
	esac
}


migrate_to_sqlite() { ## migrate-tool-install-to-sqlite: Converts normal potsgres toolshed repository tables into the SQLite version
	handle_help "$@" <<-EOF
		    $ gxadmin migrate-tool-install-to-sqlite
		    Creating new sqlite database: galaxy_install.sqlite
		    Migrating tables
		      export: tool_shed_repository
		      import: tool_shed_repository
		      ...
		      export: repository_repository_dependency_association
		      import: repository_repository_dependency_association
		    Complete
	EOF

	# Export tables
	if [[ -f  galaxy_install.sqlite ]]; then
		error "galaxy_install.sqlite exists, not overwriting"
		exit 1
	fi

	success "Creating new sqlite database: galaxy_install.sqlite"
	empty_schema=$(mktemp)
	echo "
	PRAGMA foreign_keys=OFF;
	BEGIN TRANSACTION;
	CREATE TABLE migrate_version (
		repository_id VARCHAR(250) NOT NULL,
		repository_path TEXT,
		version INTEGER,
		PRIMARY KEY (repository_id)
	);
	CREATE TABLE tool_shed_repository (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed VARCHAR(255),
		name VARCHAR(255),
		description TEXT,
		owner VARCHAR(255),
		changeset_revision VARCHAR(255),
		deleted BOOLEAN,
		metadata BLOB,
		includes_datatypes BOOLEAN,
		installed_changeset_revision VARCHAR(255),
		uninstalled BOOLEAN,
		dist_to_shed BOOLEAN,
		ctx_rev VARCHAR(10),
		status VARCHAR(255),
		error_message TEXT,
		tool_shed_status BLOB,
		PRIMARY KEY (id),
		CHECK (deleted IN (0, 1))
	);
	CREATE TABLE tool_version (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_id VARCHAR(255),
		tool_shed_repository_id INTEGER,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE tool_version_association (
		id INTEGER NOT NULL,
		tool_id INTEGER NOT NULL,
		parent_id INTEGER NOT NULL,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_id) REFERENCES tool_version (id),
		FOREIGN KEY(parent_id) REFERENCES tool_version (id)
	);
	CREATE TABLE migrate_tools (
		repository_id VARCHAR(255),
		repository_path TEXT,
		version INTEGER
	);
	CREATE TABLE tool_dependency (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER NOT NULL,
		name VARCHAR(255),
		version VARCHAR(40),
		type VARCHAR(40),
		status VARCHAR(255),
		error_message TEXT,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE repository_dependency (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER NOT NULL,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE repository_repository_dependency_association (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER,
		repository_dependency_id INTEGER,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id),
		FOREIGN KEY(repository_dependency_id) REFERENCES repository_dependency (id)
	);
	CREATE INDEX ix_tool_shed_repository_name ON tool_shed_repository (name);
	CREATE INDEX ix_tool_shed_repository_deleted ON tool_shed_repository (deleted);
	CREATE INDEX ix_tool_shed_repository_tool_shed ON tool_shed_repository (tool_shed);
	CREATE INDEX ix_tool_shed_repository_changeset_revision ON tool_shed_repository (changeset_revision);
	CREATE INDEX ix_tool_shed_repository_owner ON tool_shed_repository (owner);
	CREATE INDEX ix_tool_shed_repository_includes_datatypes ON tool_shed_repository (includes_datatypes);
	CREATE INDEX ix_tool_version_tool_shed_repository_id ON tool_version (tool_shed_repository_id);
	CREATE INDEX ix_tool_version_association_tool_id ON tool_version_association (tool_id);
	CREATE INDEX ix_tool_version_association_parent_id ON tool_version_association (parent_id);
	CREATE INDEX ix_tool_dependency_tool_shed_repository_id ON tool_dependency (tool_shed_repository_id);
	CREATE INDEX ix_repository_dependency_tool_shed_repository_id ON repository_dependency (tool_shed_repository_id);
	CREATE INDEX ix_repository_repository_dependency_association_tool_shed_repository_id ON repository_repository_dependency_association (tool_shed_repository_id);
	CREATE INDEX ix_repository_repository_dependency_association_repository_dependency_id ON repository_repository_dependency_association (repository_dependency_id);
	COMMIT;
	" > ${empty_schema}
	sqlite3 galaxy_install.sqlite < ${empty_schema}
	rm ${empty_schema}

	success "Migrating tables"


	# tool_shed_repository is special :(
	table=tool_shed_repository
	success "  export: ${table}"
	export_csv=$(mktemp /tmp/tmp.gxadmin.${table}.XXXXXXXXXXX)
	psql -c "COPY (select
		id, create_time, update_time, tool_shed, name, description, owner, changeset_revision, case when deleted then 1 else 0 end, metadata, includes_datatypes, installed_changeset_revision, uninstalled, dist_to_shed, ctx_rev, status, error_message, tool_shed_status from $table) to STDOUT with CSV" > $export_csv;

	success "  import: ${table}"
	echo ".mode csv
.import ${export_csv} ${table}" | sqlite3 galaxy_install.sqlite
	if (( $? == 0 )); then
		rm ${export_csv}
	else
		error "  sql: ${export_csv}"
	fi

	sqlite3 galaxy_install.sqlite "insert into migrate_version values ('ToolShedInstall', 'lib/galaxy/model/tool_shed_install/migrate', 17)"
	# the rest are sane!
	for table in {tool_version,tool_version_association,migrate_tools,tool_dependency,repository_dependency,repository_repository_dependency_association}; do
		success "  export: ${table}"
		export_csv=$(mktemp /tmp/tmp.gxadmin.${table}.XXXXXXXXXXX)
		psql -c "COPY (select * from $table) to STDOUT with CSV" > $export_csv;

		success "  import: ${table}"
		echo ".mode csv
.import ${export_csv} ${table}" | sqlite3 galaxy_install.sqlite
		if (( $? == 0 )); then
			rm ${export_csv}
		else
			error "  sql: ${export_csv}"
		fi
	done

	success "Complete"
}

query_latest_users() { ## query latest-users: 40 recently registered users
	handle_help "$@" <<-EOF
		Returns 40 most recently registered users

		    $ gxadmin query latest-users
		     id |        create_time        | pg_size_pretty |   username    |             email
		    ----+---------------------------+----------------+---------------+--------------------------------
		      1 | 2018-10-05 11:40:42.90119 |                | helena-rasche | hxr@informatik.uni-freiburg.de
	EOF

	read -r -d '' QUERY <<-EOF
		SELECT
			id,
			create_time,
			pg_size_pretty(disk_usage) as disk_usage,
			username,
			email,
			array_to_string(ARRAY(
				select galaxy_group.name from galaxy_group where id in (
					select group_id from user_group_association where user_group_association.user_id = galaxy_user.id
				)
			), ' ') as groups,
			active
		FROM galaxy_user
		ORDER BY create_time desc
		LIMIT 40
	EOF
}

query_tool_usage() { ## query tool-usage: Counts of tool runs
	handle_help "$@" <<-EOF
		    $ gxadmin tool-usage
		                                    tool_id                                 | count
		    ------------------------------------------------------------------------+--------
		     toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/1.1.0  | 958154
		     Grouping1                                                              | 638890
		     toolshed.g2.bx.psu.edu/repos/devteam/intersect/gops_intersect_1/1.0.0  | 326959
		     toolshed.g2.bx.psu.edu/repos/devteam/get_flanks/get_flanks1/1.0.0      | 320236
		     addValue                                                               | 313470
		     toolshed.g2.bx.psu.edu/repos/devteam/join/gops_join_1/1.0.0            | 312735
		     upload1                                                                | 103595
		     toolshed.g2.bx.psu.edu/repos/rnateam/graphclust_nspdk/nspdk_sparse/9.2 |  52861
		     Filter1                                                                |  43253
	EOF

	read -r -d '' QUERY <<-EOF
		SELECT
			j.tool_id, count(*) AS count
		FROM job j
		GROUP BY j.tool_id
		ORDER BY count DESC
	EOF
}

query_job_info() { ## query job-info <id> [id] ...: Information about a specific job
	handle_help "$@" <<-EOF
		    $ gxadmin query job-info 1
		     tool_id | state | username |        create_time         | job_runner_name | job_runner_external_id
		    ---------+-------+----------+----------------------------+-----------------+------------------------
		     upload1 | ok    | admin    | 2012-12-06 16:34:27.492711 | local:///       | 9347
	EOF

	assert_count_ge $# 1 "Missing Job ID"
	job_ids="$(echo $@ | sed 's/\s\+/,/g')"

	read -r -d '' QUERY <<-EOF
		SELECT job.id, job.tool_id, job.state, job.handler, galaxy_user.username, job.create_time, job.job_runner_name, job.job_runner_external_id
		FROM job, galaxy_user
		WHERE job.id in ($job_ids) AND job.user_id = galaxy_user.id
	EOF
}

query_datasets_created_daily() { ## query datasets-created-daily: The min/max/average/p95/p99 of total size of datasets created in a single day.
	handle_help "$@" <<-EOF
		    $ gxadmin query datasets-created-daily
		       min   |  avg   | perc_95 | perc_99 |  max
		    ---------+--------+---------+---------+-------
		     0 bytes | 338 GB | 1355 GB | 2384 GB | 42 TB
	EOF

	read -r -d '' QUERY <<-EOF
		WITH temp_queue_times AS
		(select
			date_trunc('day', create_time),
			sum(total_size)
		from dataset
		group by date_trunc
		order by date_trunc desc)
		select
			pg_size_pretty(min(sum)) as min,
			pg_size_pretty(avg(sum)) as avg,
			pg_size_pretty(percentile_cont(0.95) WITHIN GROUP (ORDER BY sum) ::bigint) as perc_95,
			pg_size_pretty(percentile_cont(0.99) WITHIN GROUP (ORDER BY sum) ::bigint) as perc_99,
			pg_size_pretty(max(sum)) as max
		from temp_queue_times
	EOF
}

query_largest_collection() { ## query largest-collection: Returns the size of the single largest collection
	handle_help "$@" <<-EOF
	EOF

	read -r -d '' QUERY <<-EOF
		WITH asdf as (select count(*) from dataset_collection_element group by dataset_collection_id order by count desc) select max(count) from asdf;
	EOF
}

query_queue_time() { ## query queue-time <tool_id>: The average/95%/99% a specific tool spends in queue state.
	handle_help "$@" <<-EOF
		    $ gxadmin query queue-time toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_geneBody_coverage/2.6.4.3
		           min       |     perc_95     |     perc_99     |       max
		    -----------------+-----------------+-----------------+-----------------
		     00:00:15.421457 | 00:00:55.022874 | 00:00:59.974171 | 00:01:01.211995
	EOF

	assert_count $# 1 "Missing tool ID"

	read -r -d '' QUERY <<-EOF
		WITH temp_queue_times AS
		(select
			min(a.create_time - b.create_time) as queue_time
		from
			job_state_history as a
		inner join
			job_state_history as b
		on
			(a.job_id = b.job_id)
		where
			a.job_id in (select id from job where tool_id like '%$3%' and state = 'ok' and create_time > (now() - '3 months'::interval))
			and a.state = 'running'
			and b.state = 'queued'
		group by
			a.job_id
		order by
			queue_time desc
		)
		select
			min(queue_time),
			percentile_cont(0.95) WITHIN GROUP (ORDER BY queue_time) as perc_95,
			percentile_cont(0.99) WITHIN GROUP (ORDER BY queue_time) as perc_99,
			max(queue_time)
		from temp_queue_times
	EOF
}

query_queue() { ## query queue: Brief overview of currently running jobs
	handle_help "$@" <<-EOF
		    $ gxadmin query queue
		                                tool_id                                |  state  | count
		    -------------------------------------------------------------------+---------+-------
		     toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0      | queued  |     9
		     toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0     | running |     7
		     toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                | queued  |     6
		     ebi_sra_main                                                      | running |     6
		     toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.8.3            | queued  |     5
		     toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.3.4.2      | running |     5
		     toolshed.g2.bx.psu.edu/repos/nml/spades/spades/3.11.1+galaxy1     | queued  |     4
		     toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0 | running |     2
		     toolshed.g2.bx.psu.edu/repos/nml/metaspades/metaspades/3.9.0      | running |     2
		     upload1                                                           | running |     2
	EOF

	read -r -d '' QUERY <<-EOF
			SELECT tool_id, state, count(*)
			FROM job
			WHERE state in ('queued', 'running')
			GROUP BY tool_id, state
			ORDER BY count desc
	EOF
}

query_queue_overview() { ## query queue-overview: View used mostly for monitoring
	handle_help "$@" <<-EOF
		Primarily for monitoring of queue. Optimally used with 'iquery' and passed to Telegraf.

		    $ gxadmin iquery queue-overview
		    queue-overview,tool_id=test_history_sanitization,tool_version=0.0.1,state=running,handler=main.web.1,destination_id=condor,job_runner_name=condor count=1

	EOF

	fields="count=6"
	tags="tool_id=0;tool_version=1;destination_id=2;handler=3;state=4;job_runner_name=5"
	read -r -d '' QUERY <<-EOF
		SELECT
			tool_id, tool_version, destination_id, handler, state, job_runner_name, count(*) as count
		FROM job
		WHERE
			state = 'running' or state = 'queued'
		GROUP BY
			tool_id, tool_version, destination_id, handler, state, job_runner_name
	EOF
}

query_queue_detail() { ## query queue-detail [--all]: Detailed overview of running and queued jobs
	handle_help "$@" <<-EOF
		    $ gxadmin query queue-detail
		      state  |   id    |  extid  |                                 tool_id                                   |      username       | time_since_creation
		    ---------+---------+---------+---------------------------------------------------------------------------+---------------------+---------------------
		     running | 4360629 | 229333  | toolshed.g2.bx.psu.edu/repos/bgruening/infernal/infernal_cmsearch/1.1.2.0 |                     | 5 days 11:00:00
		     running | 4362676 | 230237  | toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0         |                     | 4 days 18:00:00
		     running | 4364499 | 231055  | toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0         |                     | 4 days 05:00:00
		     running | 4366604 | 5183013 | toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0             |                     | 3 days 20:00:00
		     running | 4366605 | 5183016 | toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0             |                     | 3 days 20:00:00
		     queued  | 4350274 | 225743  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              |                     | 9 days 05:00:00
		     queued  | 4353435 | 227038  | toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.8.3                    |                     | 8 days 08:00:00
		     queued  | 4361914 | 229712  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              |                     | 5 days -01:00:00
		     queued  | 4361812 | 229696  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              |                     | 5 days -01:00:00
		     queued  | 4361939 | 229728  | toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                        |                     | 4 days 21:00:00
		     queued  | 4361941 | 229731  | toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                        |                     | 4 days 21:00:00
	EOF

	d=""
	if [[ $1 == "--all" ]]; then
		d=", 'new'"
	fi

		read -r -d '' QUERY <<-EOF
			SELECT job.state, job.id, job.job_runner_external_id as extid, job.tool_id, galaxy_user.username, date_trunc('hour', (now() - job.create_time - '2 hours'::interval)) as time_since_creation
			FROM job, galaxy_user
			WHERE state in ('running', 'queued'$d) and job.user_id = galaxy_user.id
			ORDER BY state desc, time_since_creation desc
	EOF
}

query_runtime_per_user() { ## query runtime-per-user <email>: computation time of user (by email)
	handle_help "$@" <<-EOF
		    $ gxadmin query runtime-per-user hxr@informatik.uni-freiburg.de
		       sum
		    ----------
		     14:07:39
	EOF

	assert_count $# 1 "Missing user"

	read -r -d '' QUERY <<-EOF
			SELECT sum((metric_value || ' second')::interval)
			FROM job_metric_numeric
			WHERE job_id in (
				SELECT id
				FROM job
				WHERE user_id in (
					SELECT id
					FROM galaxy_user
					where email = '$1'
				)
			) AND metric_name = 'runtime_seconds'
	EOF
}

query_jobs_per_user() { ## query jobs-per-user <email>: Number of jobs run by a specific user
	handle_help "$@" <<-EOF
		    $ gxadmin query jobs-per-user hxr@informatik.uni-freiburg.de
		     count
		    -------
		      1460
	EOF

	assert_count $# 1 "Missing user"
	read -r -d '' QUERY <<-EOF
			SELECT count(id)
			FROM job
			WHERE user_id in (
				SELECT id
				FROM galaxy_user
				WHERE email = '$1'
	EOF
}

query_recent_jobs() { ## query recent-jobs <hours>: Jobs run in the past <hours> (in any state)
	handle_help "$@" <<-EOF
		Note that your database may have a different TZ than your querying. This is probably a misconfiguration on our end, please let me know how to fix it. Just add your offset to UTC to your query.

		    $ gxadmin query recent-jobs 2.1
		       id    |     date_trunc      |      tool_id          | state |    username
		    ---------+---------------------+-----------------------+-------+-----------------
		     4383997 | 2018-10-05 16:07:00 | Filter1               | ok    |
		     4383994 | 2018-10-05 16:04:00 | echo_main_condor      | ok    |
		     4383993 | 2018-10-05 16:04:00 | echo_main_drmaa       | error |
		     4383992 | 2018-10-05 16:04:00 | echo_main_handler11   | ok    |
		     4383983 | 2018-10-05 16:04:00 | echo_main_handler2    | ok    |
		     4383982 | 2018-10-05 16:04:00 | echo_main_handler1    | ok    |
		     4383981 | 2018-10-05 16:04:00 | echo_main_handler0    | ok    |
	EOF

	assert_count $# 1 "Missing hours"

	read -r -d '' QUERY <<-EOF
			SELECT
				job.id, date_trunc('minute', job.create_time), job.tool_id, job.state, galaxy_user.username
			FROM job, galaxy_user
			WHERE job.create_time > (now() - '$1 hours'::interval) AND job.user_id = galaxy_user.id
			ORDER BY id desc
	EOF
}

query_training() { ## query training [--all]: List known trainings
	handle_help "$@" <<-EOF
		This module is specific to EU's implementation of Training Infrastructure as a Service. But this specifically just checks for all groups with the name prefix `training-`

		    $ gxadmin query training
		           name       |  created
		    ------------------+------------
		     hts2018          | 2018-09-19
	EOF

	d1=""
	d2="AND deleted = false"
	if [[ $1 == "--all" ]]; then
		d1=", deleted"
		d2=""
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			substring(name from 10) as name,
			date_trunc('day', create_time)::date as created
			$d1
		FROM galaxy_group
		WHERE name like 'training-%' $d2
		ORDER BY create_time DESC
	EOF
}

query_training_members() { ## query training-members <tr_id>: List users in a specific training
	handle_help "$@" <<-EOF
		    $ gxadmin query training-members hts2018
		          username      |       joined
		    --------------------+---------------------
		     helena-rasche      | 2018-09-21 21:42:01
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	read -r -d '' QUERY <<-EOF
			SELECT
				galaxy_user.username,
				date_trunc('second', user_group_association.create_time) as joined
			FROM galaxy_user, user_group_association, galaxy_group
			WHERE galaxy_group.name = 'training-$ww'
				AND galaxy_group.id = user_group_association.group_id
				AND user_group_association.user_id = galaxy_user.id
	EOF
}

query_training_memberof() { ## query training-memberof <username>: List trainings that a user is part of
	handle_help "$@" <<-EOF
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	read -r -d '' QUERY <<-EOF
			SELECT
                galaxy_group.name
			FROM
                galaxy_user, user_group_association, galaxy_group
			WHERE   galaxy_group.id = user_group_association.group_id
				AND user_group_association.user_id = galaxy_user.id
                AND galaxy_user.username='$ww'
	EOF
}

query_training_remove_member() { ## query training-remove-member <training> <username> [YESDOIT]: Remove a user from a training
	handle_help "$@" <<-EOF
	EOF

	assert_count_ge $# 2 "Missing parameters"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	if (( $# == 3 )) && [[ "$3" == "YESDOIT" ]]; then
		results="$(query_tsv "$qstr")"
		uga_id=$(echo "$results" | awk -F'\t' '{print $1}')
		if (( uga_id > -1 )); then
			qstr="delete from user_group_association where id = $uga_id"
		fi
		echo $qstr
	else
		read -r -d '' QUERY <<-EOF
			SELECT
				user_group_association.id,
				galaxy_user.username,
				galaxy_group.name
			FROM
				user_group_association
			LEFT JOIN galaxy_user ON user_group_association.user_id = galaxy_user.id
			LEFT JOIN galaxy_group ON galaxy_group.id = user_group_association.group_id
			WHERE
				galaxy_group.name = 'training-$ww'
				AND galaxy_user.username = '$2'
		EOF
	fi
}

query_training_queue() { ## query training-queue <training_id>: Jobs currently being run by people in a given training
	handle_help "$@" <<-EOF
		Finds all jobs by people in that queue (including things they are executing that are not part of a training)

		    $ gxadmin query training-queue hts2018
		     state  |   id    | extid  | tool_id |   username    |       created
		    --------+---------+--------+---------+---------------+---------------------
		     queued | 4350274 | 225743 | upload1 |               | 2018-09-26 10:00:00
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	read -r -d '' QUERY <<-EOF
			SELECT
				job.state,
				job.id,
				job.job_runner_external_id AS extid,
				job.tool_id,
				galaxy_user.username,
				date_trunc('hour', job.create_time) AS created
			FROM
				job, galaxy_user
			WHERE
				state IN ('running', 'queued', 'new')
				AND job.user_id = galaxy_user.id
				AND galaxy_user.id
					IN (
							SELECT
								galaxy_user.id
							FROM
								galaxy_user, user_group_association, galaxy_group
							WHERE
								galaxy_group.name = 'training-$ww'
								AND galaxy_group.id = user_group_association.group_id
								AND user_group_association.user_id = galaxy_user.id
						)
			ORDER BY
				state DESC, created DESC
	EOF
}

query_disk_usage() { ## query disk-usage: Disk usage per object store.
	handle_help "$@" <<-EOF
		TODO: implement flag for --nice numbers

		     object_store_id |      sum
		    -----------------+----------------
		     files8          | 88109503720067
		     files6          | 64083627169725
		     files9          | 53690953947700
		     files7          | 30657241908566
		     files1          | 30633153627407
		     files2          | 22117477087642
		     files3          | 21571951600351
		     files4          | 13969690603365
		                     |  6943415154832
		     secondary       |   594632335718
		    (10 rows)
	EOF

	read -r -d '' QUERY <<-EOF
			SELECT
				object_store_id, sum(total_size)
			FROM dataset
			WHERE NOT purged
			GROUP BY object_store_id
			ORDER BY sum DESC
	EOF
}

query_users_count() { ## query users-count: Shows sums of active/external/deleted/purged accounts
	handle_help "$@" <<-EOF
		     active | external | deleted | purged | count
		    --------+----------+---------+--------+-------
		     f      | f        | f       | f      |   182
		     t      | f        | t       | t      |     2
		     t      | f        | t       | f      |     6
		     t      | f        | f       | f      |  2350
		     f      | f        | t       | t      |    36
	EOF

	read -r -d '' QUERY <<-EOF
			SELECT
				active, external, deleted, purged, count(*) as count
			FROM
				galaxy_user
			GROUP BY
				active, external, deleted, purged
	EOF
}

query_tool_last_used_date() { ## query tool-last-used: When was the most recent invocation of every tool
	handle_help "@" <<-EOF
		It is not truly every tool, there is no easy way to find the tools which have never been run.
	EOF

	read -r -d '' QUERY <<-EOF
		select max(date_trunc('month', create_time)), tool_id from job group by tool_id order by max desc;
	EOF
}

query_users_total() { ## query users-total: Total number of Galaxy users (incl deleted, purged, inactive)
	handle_help "$@" <<-EOF
	EOF
	read -r -d '' QUERY <<-EOF
			SELECT count(*) FROM galaxy_user
	EOF
}

query_groups_list() { ## query groups-list: List all groups known to Galaxy
	handle_help "$@" <<-EOF
	EOF
	read -r -d '' QUERY <<-EOF
			SELECT
				galaxy_group.name, count(*)
			FROM
				galaxy_group, user_group_association
			WHERE
				user_group_association.group_id = galaxy_group.id
			GROUP BY name
	EOF
}

query_collection_usage() { ## query collection-usage: Information about how many collections of various types are used
	handle_help "$@" <<-EOF
	EOF
	read -r -d '' QUERY <<-EOF
			SELECT
				collection_type, count(*)
			FROM
				dataset_collection
			GROUP BY
				collection_type
	EOF
}

query_ts_repos() { ## query ts-repos: Counts of toolshed repositories by toolshed and owner.
	handle_help "$@" <<-EOF
	EOF
	read -r -d '' QUERY <<-EOF
			SELECT
				tool_shed, owner, count(*)
			FROM
				tool_shed_repository
			GROUP BY
				tool_shed, owner
	EOF
}

query_active_users() { ## query active-users [weeks]: Count of users who ran jobs in past 1 week (default = 1)
	handle_help "$@" <<-EOF
		Unique users who ran jobs in past week:

		    $ gxadmin query active-users
		     count
		    -------
		       220
		    (1 row)

		Or the monthly-active-users:

		    $ gxadmin query active-users 4
		     count
		    -------
		       555
		    (1 row)
	EOF

	weeks=1
	if (( $# > 0 )); then
		weeks=$1
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			count(distinct user_id)
		FROM
			job
		WHERE
			job.create_time > (now() - '$weeks weeks'::interval)
	EOF
}

query_job_history() { ## query job-history <id>: Job state history for a specific job
	handle_help "$@" <<-EOF
		    $ gxadmin query job-history 4384025
		            time         |  state
		    ---------------------+---------
		     2018-10-05 16:20:13 | ok
		     2018-10-05 16:19:57 | running
		     2018-10-05 16:19:55 | queued
		     2018-10-05 16:19:54 | new
		    (4 rows)
	EOF

	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				date_trunc('second', create_time) as time,
				state
			FROM job_state_history
			WHERE job_id = $1
	EOF
}

query_job_inputs() { ## query job-inputs <id>: Input datasets to a specific job
	handle_help "$@" <<-EOF
	EOF
	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				hda.id AS hda_id,
				hda.state AS hda_state,
				hda.deleted AS hda_deleted,
				hda.purged AS hda_purged,
				d.id AS d_id,
				d.state AS d_state,
				d.deleted AS d_deleted,
				d.purged AS d_purged,
				d.object_store_id AS object_store_id
			FROM job j
				JOIN job_to_input_dataset jtid
					ON j.id = jtid.job_id
				JOIN history_dataset_association hda
					ON hda.id = jtid.dataset_id
				JOIN dataset d
					ON hda.dataset_id = d.id
			WHERE j.id = $1
	EOF
}

query_job_outputs() { ## query job-outputs <id>: Output datasets from a specific job
	handle_help "$@" <<-EOF
	EOF

	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				hda.id AS hda_id,
				hda.state AS hda_state,
				hda.deleted AS hda_deleted,
				hda.purged AS hda_purged,
				d.id AS d_id,
				d.state AS d_state,
				d.deleted AS d_deleted,
				d.purged AS d_purged,
				d.object_store_id AS object_store_id
			FROM job j
				JOIN job_to_output_dataset jtod
					ON j.id = jtod.job_id
				JOIN history_dataset_association hda
					ON hda.id = jtod.dataset_id
				JOIN dataset d
					ON hda.dataset_id = d.id
			WHERE j.id = $1
		"
	EOF
}

update() { ## update: Update the script
	handle_help "$@" <<-EOF
	EOF

	tmp=$(mktemp);
	curl https://raw.githubusercontent.com/usegalaxy-eu/gxadmin/master/gxadmin > $tmp;
	chmod ugo+x $tmp;
	chmod ugo+r $tmp;
	mv $tmp $0;
	exit 0
}

version() {
	echo 9
}


cmdlist() {
	IFS=$'\n'
	# TOC
	echo "Command | Description"
	echo "------- | -----------"
	for command in $(grep -o ' ## .*' $0 | grep -v grep | sort | sed 's/^ ## //g'); do
		cmd_part="$(echo $command | sed 's/:.*//g;s/\s*<.*//g;s/\s*\[.*//')"
		desc_part="$(echo $command | sed 's/^[^:]*:\s*//g')"
		key_part="$(echo $cmd_part | sed 's/ /-/g')"
		echo "[${cmd_part}](#${key_part}) | $desc_part"
	done
	echo

	# Now for sections
	for command in $(grep -o ' ## .*' $0 | grep -v grep | sort | sed 's/^ ## //g'); do
		cmd_part="$(echo $command | sed 's/:.*//g;s/\s*<.*//g;s/\s*\[.*//')"
		echo
		echo "### $cmd_part"
		echo
		bash -c "$0 $cmd_part --help"
	done

}


obtain_query() {
	query_name="$1"; shift

	case "$query_name" in
		latest-users           )  query_latest_users           "$@" ;;
		tool-usage             )  query_tool_usage             "$@" ;;
		job-info               )  query_job_info               "$@" ;;
		datasets-created-daily )  query_datasets_created_daily "$@" ;;
		queue-time             )  query_queue_time             "$@" ;;
		largest-collection     )  query_largest_collection     "$@" ;;
		queue                  )  query_queue                  "$@" ;;
		queue-detail           )  query_queue_detail           "$@" ;;
		queue-overview         )  query_queue_overview         "$@" ;;
		runtime-per-user       )  query_runtime_per_user       "$@" ;;
		jobs-per-user          )  query_jobs_per_user          "$@" ;;
		recent-jobs            )  query_recent_jobs            "$@" ;;
		training               )  query_training               "$@" ;;
		training-members       )  query_training_members       "$@" ;;
		training-memberof      )  query_training_memberof      "$@" ;;
		training-remove-member )  query_training_remove_member "$@" ;;
		training-queue         )  query_training_queue         "$@" ;;
		tool-last-used-date    )  query_tool_last_used_date    "$@" ;;
		disk-usage             )  query_disk_usage             "$@" ;;
		users-count            )  query_users_count            "$@" ;;
		users-total            )  query_users_total            "$@" ;;
		groups-list            )  query_groups_list            "$@" ;;
		collection-usage       )  query_collection_usage       "$@" ;;
		ts-repos               )  query_ts_repos               "$@" ;;
		active-users           )  query_active_users           "$@" ;;
		job-history            )  query_job_history            "$@" ;;
		job-inputs             )  query_job_inputs             "$@" ;;
		job-outputs            )  query_job_outputs            "$@" ;;

		# default
		* ) echo "ERROR" ;;
	esac
}

query() {
	# do the thing zhu li
	query_type="$1"; shift
	subfunc="$1"; shift

	# We do not run this in a subshell because we need to "return" multiple things.
	obtain_query $subfunc "$@"

	# TODO(hxr)
	#ec=$?
	#if (( ec > 0 )); then
		#echo "$db_query"
		#exit 0
	#fi
	if [[ "$QUERY" == "ERROR" ]]; then
		usage
	fi

	# Run the queries
	case "$query_type" in
		tsvquery )  query_tsv "$QUERY";;
		csvquery )  query_csv "$QUERY";;
		query    )  query_tbl "$QUERY";;
		iquery   )  query_influx "$QUERY" "$subfunc" "$fields" "$tags";;
		# default
		*        )  usage "Error";;
	esac
}


mode="$1"; shift

case "$mode" in
	validate                       ) func_validate "$@" ;;
	cleanup                        ) cleanup "$@" ;;
	zerg                           ) zerg "$@" ;;
	handler                        ) handler "$@" ;;
	migrate-tool-install-to-sqlite ) migrate_to_sqlite "$@" ;;
	update                         ) update "$@" ;;
    *query                         ) query "$mode" "$@"    ;;

	# Generate for readme:
	cmdlist   ) cmdlist ;;
	# version commands
	version   ) version ;;
	-v        ) version ;;
	--version ) version ;;
	# help
	help      ) usage safe ;;
	-h        ) usage safe ;;
	--help    ) usage safe ;;
esac
